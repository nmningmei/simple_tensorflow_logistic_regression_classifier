{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple logistic regression implemented in tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZL6LiwwW96he9vflu6/fD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmningmei/simple_tensorflow_logistic_regression_classifier/blob/main/simple_logistic_regression_implemented_in_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "lr-BTjcucr8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/simple_tensorflow_logistic_regression_classifier\"):\n",
        "    !git clone https://github.com/nmningmei/simple_tensorflow_logistic_regression_classifier.git\n",
        "\n",
        "os.chdir(\"/content/simple_tensorflow_logistic_regression_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFgOMeqofD0X",
        "outputId": "868d8159-ee71-4981-a3c2-b761a569a30e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE                                                     temp.h5\n",
            "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/                                                test.py\n",
            "README.md                                                   utils.py\n",
            "simple_logistic_regression_implemented_in_tensorflow.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from utils import (build_logistic_regression,\n",
        "                   compile_logistic_regression)\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "sYY0s-U4e5Bt"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# experiment control"
      ],
      "metadata": {
        "id": "MCOodYUBft04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = int(1e3) # just a large number\n",
        "print_train = True"
      ],
      "metadata": {
        "id": "YZndB6f3e59i"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clear memory states"
      ],
      "metadata": {
        "id": "NdG4FfQGfxAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Tw3hNm6ZffwA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate random test data"
      ],
      "metadata": {
        "id": "0jB7pKhif0Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = make_classification(n_samples             = 500,\n",
        "                         n_features            = 100,\n",
        "                         n_informative         = 3,\n",
        "                         n_redundant           = 10,\n",
        "                         n_classes             = 2,\n",
        "                         n_clusters_per_class  = 4,\n",
        "                         flip_y                = .01,\n",
        "                         class_sep             = .5,# how easy to separate the two classes\n",
        "                         shuffle               = True,\n",
        "                         random_state          = 12345,\n",
        "                         )"
      ],
      "metadata": {
        "id": "DCFa9C5rfiBI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# one-hot encoding for softmax"
      ],
      "metadata": {
        "id": "n2nC9Ppcf2Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.reshape((-1,1))\n",
        "y = np.hstack([y,1-y])"
      ],
      "metadata": {
        "id": "tAOp05CdfkTZ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split the data into train, validation, and test"
      ],
      "metadata": {
        "id": "ebMs-Eaff4Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test   = train_test_split(X,y,test_size = .1,random_state = 12345)\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size = .1,random_state = 12345)\n",
        "# add some 0.5 labeled data - don't use too much\n",
        "X_noise = np.random.normal(X_train.mean(),X_train.std(),size = (int(X_train.shape[0]/8),100))\n",
        "y_noise = np.array([[0.5,0.5]] * int(X_train.shape[0]/8))\n",
        "X_train = np.concatenate([X_train,X_noise])\n",
        "y_train = np.concatenate([y_train,y_noise])\n",
        "\n",
        "X_noise = np.random.normal(X_test.mean(),X_test.std(),size = (int(X_test.shape[0]/2),100))\n",
        "y_noise = np.array([[0.5,0.5]] * int(X_test.shape[0]/2))\n",
        "X_test  = np.concatenate([X_test,X_noise])\n",
        "y_test  = np.concatenate([y_test,y_noise])"
      ],
      "metadata": {
        "id": "kq47iHuYflgp"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build the model"
      ],
      "metadata": {
        "id": "qQTLEM33f6dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(12345)\n",
        "logistic_regression = build_logistic_regression(\n",
        "                        input_size              = X_train.shape[1],\n",
        "                        output_size             = 2,\n",
        "                        special                 = False,\n",
        "                        kernel_regularizer      = regularizers.L2(l2 = 1e-2),\n",
        "                        activity_regularizer    = regularizers.L1(l1 = 1e-32), # this makes the prediction sparse\n",
        "                        print_model             = True,\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiJ6x_QvfoGk",
        "outputId": "ba532933-a04f-49f6-ae7e-92f2464052cb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"logistic_regression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 100)]             0         \n",
            "                                                                 \n",
            " logistic_layer (Dense)      (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 202\n",
            "Trainable params: 202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compile the model"
      ],
      "metadata": {
        "id": "3U7ZrMh-f8Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import metrics\n",
        "logistic_regression,callbacks = compile_logistic_regression(\n",
        "                                logistic_regression,\n",
        "                                model_name      = 'temp.h5',\n",
        "                                optimizer       = None,\n",
        "                                loss_function   = None,\n",
        "                                metric          = metrics.mean_absolute_error,\n",
        "                                callbacks       = None,\n",
        "                                learning_rate   = 1e-3,\n",
        "                                tol             = 1e-4,\n",
        "                                patience        = 50, # this would make the training take more time\n",
        "                                )"
      ],
      "metadata": {
        "id": "IPgnTMRGfpW-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train and validate the model"
      ],
      "metadata": {
        "id": "5rskK3z9f9pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression.fit(\n",
        "                        X_train,\n",
        "                        y_train,\n",
        "                        batch_size      = 4,\n",
        "                        epochs          = n_epochs,\n",
        "                        verbose         = print_train,\n",
        "                        callbacks       = callbacks,\n",
        "                        validation_data = (X_valid,y_valid),\n",
        "                        shuffle         = True,\n",
        "                        #class_weight    = {0:1,1:2},# tf has this but I don't think it is the same as sklearn\n",
        "                        )\n",
        "y_pred = logistic_regression.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57HViFjhfqmg",
        "outputId": "dc6292e6-29d0-46be-b91c-4fac72c5a355"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "114/114 [==============================] - 1s 5ms/step - loss: 0.9732 - mean_absolute_error: 0.5261 - val_loss: 0.9585 - val_mean_absolute_error: 0.5632\n",
            "Epoch 2/1000\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 0.9508 - mean_absolute_error: 0.5168 - val_loss: 0.9390 - val_mean_absolute_error: 0.5546\n",
            "Epoch 3/1000\n",
            "114/114 [==============================] - 0s 3ms/step - loss: 0.9298 - mean_absolute_error: 0.5076 - val_loss: 0.9208 - val_mean_absolute_error: 0.5458\n",
            "Epoch 4/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.9101 - mean_absolute_error: 0.4983 - val_loss: 0.9034 - val_mean_absolute_error: 0.5369\n",
            "Epoch 5/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8916 - mean_absolute_error: 0.4895 - val_loss: 0.8873 - val_mean_absolute_error: 0.5281\n",
            "Epoch 6/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8744 - mean_absolute_error: 0.4809 - val_loss: 0.8722 - val_mean_absolute_error: 0.5193\n",
            "Epoch 7/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8581 - mean_absolute_error: 0.4721 - val_loss: 0.8579 - val_mean_absolute_error: 0.5106\n",
            "Epoch 8/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8430 - mean_absolute_error: 0.4641 - val_loss: 0.8445 - val_mean_absolute_error: 0.5020\n",
            "Epoch 9/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8285 - mean_absolute_error: 0.4563 - val_loss: 0.8318 - val_mean_absolute_error: 0.4936\n",
            "Epoch 10/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8149 - mean_absolute_error: 0.4485 - val_loss: 0.8199 - val_mean_absolute_error: 0.4854\n",
            "Epoch 11/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.8022 - mean_absolute_error: 0.4408 - val_loss: 0.8088 - val_mean_absolute_error: 0.4774\n",
            "Epoch 12/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7902 - mean_absolute_error: 0.4337 - val_loss: 0.7981 - val_mean_absolute_error: 0.4697\n",
            "Epoch 13/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7788 - mean_absolute_error: 0.4270 - val_loss: 0.7882 - val_mean_absolute_error: 0.4622\n",
            "Epoch 14/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7681 - mean_absolute_error: 0.4204 - val_loss: 0.7788 - val_mean_absolute_error: 0.4551\n",
            "Epoch 15/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7580 - mean_absolute_error: 0.4143 - val_loss: 0.7699 - val_mean_absolute_error: 0.4483\n",
            "Epoch 16/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7484 - mean_absolute_error: 0.4084 - val_loss: 0.7615 - val_mean_absolute_error: 0.4418\n",
            "Epoch 17/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7393 - mean_absolute_error: 0.4030 - val_loss: 0.7535 - val_mean_absolute_error: 0.4356\n",
            "Epoch 18/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7306 - mean_absolute_error: 0.3973 - val_loss: 0.7460 - val_mean_absolute_error: 0.4298\n",
            "Epoch 19/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7224 - mean_absolute_error: 0.3924 - val_loss: 0.7389 - val_mean_absolute_error: 0.4243\n",
            "Epoch 20/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7146 - mean_absolute_error: 0.3875 - val_loss: 0.7321 - val_mean_absolute_error: 0.4190\n",
            "Epoch 21/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7071 - mean_absolute_error: 0.3831 - val_loss: 0.7258 - val_mean_absolute_error: 0.4142\n",
            "Epoch 22/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.7000 - mean_absolute_error: 0.3786 - val_loss: 0.7197 - val_mean_absolute_error: 0.4095\n",
            "Epoch 23/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6933 - mean_absolute_error: 0.3742 - val_loss: 0.7140 - val_mean_absolute_error: 0.4051\n",
            "Epoch 24/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6869 - mean_absolute_error: 0.3702 - val_loss: 0.7086 - val_mean_absolute_error: 0.4010\n",
            "Epoch 25/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6808 - mean_absolute_error: 0.3663 - val_loss: 0.7035 - val_mean_absolute_error: 0.3971\n",
            "Epoch 26/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6749 - mean_absolute_error: 0.3626 - val_loss: 0.6986 - val_mean_absolute_error: 0.3935\n",
            "Epoch 27/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6693 - mean_absolute_error: 0.3590 - val_loss: 0.6940 - val_mean_absolute_error: 0.3901\n",
            "Epoch 28/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6640 - mean_absolute_error: 0.3557 - val_loss: 0.6895 - val_mean_absolute_error: 0.3869\n",
            "Epoch 29/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6589 - mean_absolute_error: 0.3522 - val_loss: 0.6853 - val_mean_absolute_error: 0.3838\n",
            "Epoch 30/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6540 - mean_absolute_error: 0.3491 - val_loss: 0.6813 - val_mean_absolute_error: 0.3809\n",
            "Epoch 31/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6493 - mean_absolute_error: 0.3462 - val_loss: 0.6776 - val_mean_absolute_error: 0.3782\n",
            "Epoch 32/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6448 - mean_absolute_error: 0.3432 - val_loss: 0.6740 - val_mean_absolute_error: 0.3756\n",
            "Epoch 33/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6405 - mean_absolute_error: 0.3404 - val_loss: 0.6705 - val_mean_absolute_error: 0.3732\n",
            "Epoch 34/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6364 - mean_absolute_error: 0.3376 - val_loss: 0.6673 - val_mean_absolute_error: 0.3709\n",
            "Epoch 35/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6324 - mean_absolute_error: 0.3350 - val_loss: 0.6642 - val_mean_absolute_error: 0.3687\n",
            "Epoch 36/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6287 - mean_absolute_error: 0.3327 - val_loss: 0.6612 - val_mean_absolute_error: 0.3667\n",
            "Epoch 37/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6250 - mean_absolute_error: 0.3303 - val_loss: 0.6584 - val_mean_absolute_error: 0.3647\n",
            "Epoch 38/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6215 - mean_absolute_error: 0.3281 - val_loss: 0.6557 - val_mean_absolute_error: 0.3629\n",
            "Epoch 39/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6181 - mean_absolute_error: 0.3259 - val_loss: 0.6531 - val_mean_absolute_error: 0.3611\n",
            "Epoch 40/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6149 - mean_absolute_error: 0.3240 - val_loss: 0.6507 - val_mean_absolute_error: 0.3594\n",
            "Epoch 41/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6117 - mean_absolute_error: 0.3219 - val_loss: 0.6483 - val_mean_absolute_error: 0.3578\n",
            "Epoch 42/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6087 - mean_absolute_error: 0.3200 - val_loss: 0.6461 - val_mean_absolute_error: 0.3562\n",
            "Epoch 43/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6058 - mean_absolute_error: 0.3183 - val_loss: 0.6439 - val_mean_absolute_error: 0.3548\n",
            "Epoch 44/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6030 - mean_absolute_error: 0.3166 - val_loss: 0.6419 - val_mean_absolute_error: 0.3534\n",
            "Epoch 45/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.6004 - mean_absolute_error: 0.3150 - val_loss: 0.6400 - val_mean_absolute_error: 0.3520\n",
            "Epoch 46/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5977 - mean_absolute_error: 0.3134 - val_loss: 0.6381 - val_mean_absolute_error: 0.3508\n",
            "Epoch 47/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5952 - mean_absolute_error: 0.3120 - val_loss: 0.6364 - val_mean_absolute_error: 0.3496\n",
            "Epoch 48/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5928 - mean_absolute_error: 0.3106 - val_loss: 0.6347 - val_mean_absolute_error: 0.3484\n",
            "Epoch 49/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5905 - mean_absolute_error: 0.3091 - val_loss: 0.6331 - val_mean_absolute_error: 0.3473\n",
            "Epoch 50/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5882 - mean_absolute_error: 0.3078 - val_loss: 0.6315 - val_mean_absolute_error: 0.3462\n",
            "Epoch 51/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5860 - mean_absolute_error: 0.3064 - val_loss: 0.6301 - val_mean_absolute_error: 0.3452\n",
            "Epoch 52/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5839 - mean_absolute_error: 0.3052 - val_loss: 0.6287 - val_mean_absolute_error: 0.3443\n",
            "Epoch 53/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5819 - mean_absolute_error: 0.3039 - val_loss: 0.6274 - val_mean_absolute_error: 0.3433\n",
            "Epoch 54/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5800 - mean_absolute_error: 0.3028 - val_loss: 0.6262 - val_mean_absolute_error: 0.3425\n",
            "Epoch 55/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5781 - mean_absolute_error: 0.3016 - val_loss: 0.6250 - val_mean_absolute_error: 0.3416\n",
            "Epoch 56/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5762 - mean_absolute_error: 0.3006 - val_loss: 0.6238 - val_mean_absolute_error: 0.3408\n",
            "Epoch 57/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5744 - mean_absolute_error: 0.2995 - val_loss: 0.6227 - val_mean_absolute_error: 0.3400\n",
            "Epoch 58/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5727 - mean_absolute_error: 0.2985 - val_loss: 0.6217 - val_mean_absolute_error: 0.3392\n",
            "Epoch 59/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5711 - mean_absolute_error: 0.2975 - val_loss: 0.6206 - val_mean_absolute_error: 0.3385\n",
            "Epoch 60/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5695 - mean_absolute_error: 0.2966 - val_loss: 0.6197 - val_mean_absolute_error: 0.3378\n",
            "Epoch 61/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5678 - mean_absolute_error: 0.2957 - val_loss: 0.6188 - val_mean_absolute_error: 0.3372\n",
            "Epoch 62/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5663 - mean_absolute_error: 0.2948 - val_loss: 0.6180 - val_mean_absolute_error: 0.3365\n",
            "Epoch 63/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5649 - mean_absolute_error: 0.2940 - val_loss: 0.6172 - val_mean_absolute_error: 0.3359\n",
            "Epoch 64/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5635 - mean_absolute_error: 0.2932 - val_loss: 0.6164 - val_mean_absolute_error: 0.3353\n",
            "Epoch 65/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5621 - mean_absolute_error: 0.2923 - val_loss: 0.6157 - val_mean_absolute_error: 0.3348\n",
            "Epoch 66/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5607 - mean_absolute_error: 0.2916 - val_loss: 0.6150 - val_mean_absolute_error: 0.3342\n",
            "Epoch 67/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5595 - mean_absolute_error: 0.2907 - val_loss: 0.6143 - val_mean_absolute_error: 0.3337\n",
            "Epoch 68/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5582 - mean_absolute_error: 0.2900 - val_loss: 0.6137 - val_mean_absolute_error: 0.3332\n",
            "Epoch 69/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5570 - mean_absolute_error: 0.2895 - val_loss: 0.6131 - val_mean_absolute_error: 0.3327\n",
            "Epoch 70/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5558 - mean_absolute_error: 0.2887 - val_loss: 0.6125 - val_mean_absolute_error: 0.3323\n",
            "Epoch 71/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5546 - mean_absolute_error: 0.2880 - val_loss: 0.6120 - val_mean_absolute_error: 0.3318\n",
            "Epoch 72/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5535 - mean_absolute_error: 0.2874 - val_loss: 0.6115 - val_mean_absolute_error: 0.3314\n",
            "Epoch 73/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5524 - mean_absolute_error: 0.2868 - val_loss: 0.6110 - val_mean_absolute_error: 0.3310\n",
            "Epoch 74/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5514 - mean_absolute_error: 0.2861 - val_loss: 0.6106 - val_mean_absolute_error: 0.3306\n",
            "Epoch 75/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5503 - mean_absolute_error: 0.2855 - val_loss: 0.6102 - val_mean_absolute_error: 0.3302\n",
            "Epoch 76/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5493 - mean_absolute_error: 0.2849 - val_loss: 0.6097 - val_mean_absolute_error: 0.3298\n",
            "Epoch 77/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5484 - mean_absolute_error: 0.2844 - val_loss: 0.6093 - val_mean_absolute_error: 0.3295\n",
            "Epoch 78/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5474 - mean_absolute_error: 0.2839 - val_loss: 0.6090 - val_mean_absolute_error: 0.3291\n",
            "Epoch 79/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5465 - mean_absolute_error: 0.2833 - val_loss: 0.6087 - val_mean_absolute_error: 0.3288\n",
            "Epoch 80/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5456 - mean_absolute_error: 0.2829 - val_loss: 0.6083 - val_mean_absolute_error: 0.3285\n",
            "Epoch 81/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5447 - mean_absolute_error: 0.2823 - val_loss: 0.6080 - val_mean_absolute_error: 0.3282\n",
            "Epoch 82/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5439 - mean_absolute_error: 0.2820 - val_loss: 0.6078 - val_mean_absolute_error: 0.3279\n",
            "Epoch 83/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5431 - mean_absolute_error: 0.2815 - val_loss: 0.6075 - val_mean_absolute_error: 0.3276\n",
            "Epoch 84/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5423 - mean_absolute_error: 0.2811 - val_loss: 0.6072 - val_mean_absolute_error: 0.3273\n",
            "Epoch 85/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5415 - mean_absolute_error: 0.2806 - val_loss: 0.6070 - val_mean_absolute_error: 0.3270\n",
            "Epoch 86/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5407 - mean_absolute_error: 0.2801 - val_loss: 0.6068 - val_mean_absolute_error: 0.3267\n",
            "Epoch 87/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5400 - mean_absolute_error: 0.2798 - val_loss: 0.6066 - val_mean_absolute_error: 0.3265\n",
            "Epoch 88/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5393 - mean_absolute_error: 0.2793 - val_loss: 0.6064 - val_mean_absolute_error: 0.3262\n",
            "Epoch 89/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5386 - mean_absolute_error: 0.2789 - val_loss: 0.6063 - val_mean_absolute_error: 0.3260\n",
            "Epoch 90/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5379 - mean_absolute_error: 0.2785 - val_loss: 0.6061 - val_mean_absolute_error: 0.3258\n",
            "Epoch 91/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5372 - mean_absolute_error: 0.2781 - val_loss: 0.6060 - val_mean_absolute_error: 0.3255\n",
            "Epoch 92/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5365 - mean_absolute_error: 0.2778 - val_loss: 0.6059 - val_mean_absolute_error: 0.3253\n",
            "Epoch 93/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5359 - mean_absolute_error: 0.2774 - val_loss: 0.6058 - val_mean_absolute_error: 0.3251\n",
            "Epoch 94/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5353 - mean_absolute_error: 0.2771 - val_loss: 0.6057 - val_mean_absolute_error: 0.3249\n",
            "Epoch 95/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5347 - mean_absolute_error: 0.2767 - val_loss: 0.6056 - val_mean_absolute_error: 0.3247\n",
            "Epoch 96/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5341 - mean_absolute_error: 0.2763 - val_loss: 0.6056 - val_mean_absolute_error: 0.3246\n",
            "Epoch 97/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5335 - mean_absolute_error: 0.2760 - val_loss: 0.6055 - val_mean_absolute_error: 0.3244\n",
            "Epoch 98/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5330 - mean_absolute_error: 0.2757 - val_loss: 0.6055 - val_mean_absolute_error: 0.3242\n",
            "Epoch 99/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5324 - mean_absolute_error: 0.2753 - val_loss: 0.6055 - val_mean_absolute_error: 0.3241\n",
            "Epoch 100/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5319 - mean_absolute_error: 0.2751 - val_loss: 0.6055 - val_mean_absolute_error: 0.3239\n",
            "Epoch 101/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5314 - mean_absolute_error: 0.2746 - val_loss: 0.6055 - val_mean_absolute_error: 0.3238\n",
            "Epoch 102/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5309 - mean_absolute_error: 0.2744 - val_loss: 0.6054 - val_mean_absolute_error: 0.3236\n",
            "Epoch 103/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5304 - mean_absolute_error: 0.2743 - val_loss: 0.6055 - val_mean_absolute_error: 0.3235\n",
            "Epoch 104/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5299 - mean_absolute_error: 0.2738 - val_loss: 0.6055 - val_mean_absolute_error: 0.3233\n",
            "Epoch 105/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5294 - mean_absolute_error: 0.2737 - val_loss: 0.6055 - val_mean_absolute_error: 0.3232\n",
            "Epoch 106/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5289 - mean_absolute_error: 0.2733 - val_loss: 0.6055 - val_mean_absolute_error: 0.3230\n",
            "Epoch 107/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5285 - mean_absolute_error: 0.2730 - val_loss: 0.6055 - val_mean_absolute_error: 0.3229\n",
            "Epoch 108/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5280 - mean_absolute_error: 0.2728 - val_loss: 0.6055 - val_mean_absolute_error: 0.3228\n",
            "Epoch 109/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5276 - mean_absolute_error: 0.2725 - val_loss: 0.6056 - val_mean_absolute_error: 0.3227\n",
            "Epoch 110/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5272 - mean_absolute_error: 0.2723 - val_loss: 0.6056 - val_mean_absolute_error: 0.3225\n",
            "Epoch 111/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5267 - mean_absolute_error: 0.2720 - val_loss: 0.6057 - val_mean_absolute_error: 0.3224\n",
            "Epoch 112/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_absolute_error: 0.2718 - val_loss: 0.6058 - val_mean_absolute_error: 0.3223\n",
            "Epoch 113/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_absolute_error: 0.2715 - val_loss: 0.6058 - val_mean_absolute_error: 0.3222\n",
            "Epoch 114/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_absolute_error: 0.2713 - val_loss: 0.6059 - val_mean_absolute_error: 0.3221\n",
            "Epoch 115/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5252 - mean_absolute_error: 0.2710 - val_loss: 0.6059 - val_mean_absolute_error: 0.3220\n",
            "Epoch 116/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5248 - mean_absolute_error: 0.2708 - val_loss: 0.6061 - val_mean_absolute_error: 0.3218\n",
            "Epoch 117/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5245 - mean_absolute_error: 0.2706 - val_loss: 0.6062 - val_mean_absolute_error: 0.3217\n",
            "Epoch 118/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5241 - mean_absolute_error: 0.2704 - val_loss: 0.6062 - val_mean_absolute_error: 0.3217\n",
            "Epoch 119/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5238 - mean_absolute_error: 0.2702 - val_loss: 0.6064 - val_mean_absolute_error: 0.3216\n",
            "Epoch 120/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5234 - mean_absolute_error: 0.2700 - val_loss: 0.6065 - val_mean_absolute_error: 0.3215\n",
            "Epoch 121/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5231 - mean_absolute_error: 0.2697 - val_loss: 0.6066 - val_mean_absolute_error: 0.3214\n",
            "Epoch 122/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5228 - mean_absolute_error: 0.2696 - val_loss: 0.6067 - val_mean_absolute_error: 0.3213\n",
            "Epoch 123/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5224 - mean_absolute_error: 0.2693 - val_loss: 0.6068 - val_mean_absolute_error: 0.3212\n",
            "Epoch 124/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5222 - mean_absolute_error: 0.2690 - val_loss: 0.6069 - val_mean_absolute_error: 0.3211\n",
            "Epoch 125/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5219 - mean_absolute_error: 0.2690 - val_loss: 0.6070 - val_mean_absolute_error: 0.3211\n",
            "Epoch 126/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5215 - mean_absolute_error: 0.2687 - val_loss: 0.6071 - val_mean_absolute_error: 0.3210\n",
            "Epoch 127/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5212 - mean_absolute_error: 0.2685 - val_loss: 0.6072 - val_mean_absolute_error: 0.3209\n",
            "Epoch 128/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5210 - mean_absolute_error: 0.2684 - val_loss: 0.6073 - val_mean_absolute_error: 0.3209\n",
            "Epoch 129/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5207 - mean_absolute_error: 0.2681 - val_loss: 0.6075 - val_mean_absolute_error: 0.3208\n",
            "Epoch 130/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5204 - mean_absolute_error: 0.2679 - val_loss: 0.6076 - val_mean_absolute_error: 0.3208\n",
            "Epoch 131/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5201 - mean_absolute_error: 0.2678 - val_loss: 0.6078 - val_mean_absolute_error: 0.3207\n",
            "Epoch 132/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5198 - mean_absolute_error: 0.2676 - val_loss: 0.6079 - val_mean_absolute_error: 0.3206\n",
            "Epoch 133/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5196 - mean_absolute_error: 0.2675 - val_loss: 0.6081 - val_mean_absolute_error: 0.3206\n",
            "Epoch 134/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5194 - mean_absolute_error: 0.2673 - val_loss: 0.6083 - val_mean_absolute_error: 0.3205\n",
            "Epoch 135/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5191 - mean_absolute_error: 0.2671 - val_loss: 0.6084 - val_mean_absolute_error: 0.3205\n",
            "Epoch 136/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5188 - mean_absolute_error: 0.2669 - val_loss: 0.6086 - val_mean_absolute_error: 0.3204\n",
            "Epoch 137/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5186 - mean_absolute_error: 0.2668 - val_loss: 0.6088 - val_mean_absolute_error: 0.3204\n",
            "Epoch 138/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5184 - mean_absolute_error: 0.2667 - val_loss: 0.6089 - val_mean_absolute_error: 0.3203\n",
            "Epoch 139/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5181 - mean_absolute_error: 0.2665 - val_loss: 0.6091 - val_mean_absolute_error: 0.3203\n",
            "Epoch 140/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5180 - mean_absolute_error: 0.2664 - val_loss: 0.6093 - val_mean_absolute_error: 0.3203\n",
            "Epoch 141/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5177 - mean_absolute_error: 0.2662 - val_loss: 0.6094 - val_mean_absolute_error: 0.3202\n",
            "Epoch 142/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5175 - mean_absolute_error: 0.2661 - val_loss: 0.6096 - val_mean_absolute_error: 0.3202\n",
            "Epoch 143/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5173 - mean_absolute_error: 0.2660 - val_loss: 0.6098 - val_mean_absolute_error: 0.3201\n",
            "Epoch 144/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5171 - mean_absolute_error: 0.2657 - val_loss: 0.6100 - val_mean_absolute_error: 0.3201\n",
            "Epoch 145/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5168 - mean_absolute_error: 0.2657 - val_loss: 0.6102 - val_mean_absolute_error: 0.3200\n",
            "Epoch 146/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5167 - mean_absolute_error: 0.2656 - val_loss: 0.6104 - val_mean_absolute_error: 0.3200\n",
            "Epoch 147/1000\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.5165 - mean_absolute_error: 0.2654 - val_loss: 0.6105 - val_mean_absolute_error: 0.3200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_idx, = np.where(y_test[:,-1] != 0.5)\n",
        "print(f'test score = {roc_auc_score(y_test[_idx],y_pred[_idx],):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fs-lPu4ge2G",
        "outputId": "3149de5d-44ae-4cfe-bdcc-66d957aa2688"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test score = 0.7101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualize the distribution of the predictions"
      ],
      "metadata": {
        "id": "-xkyrZVbf_cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5ACRnEGhW-z",
        "outputId": "1bbd304a-115b-4455-da4c-1bda3df79276"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.  0. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [1.  0. ]\n",
            " [0.  1. ]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "[[0.9917195  0.00828051]\n",
            " [0.02096721 0.9790329 ]\n",
            " [0.7014006  0.29859945]\n",
            " [0.7677439  0.2322561 ]\n",
            " [0.9974952  0.00250483]\n",
            " [0.37727454 0.6227254 ]\n",
            " [0.9334753  0.0665247 ]\n",
            " [0.6115866  0.38841346]\n",
            " [0.9346065  0.06539353]\n",
            " [0.00291776 0.9970822 ]\n",
            " [0.2820816  0.7179184 ]\n",
            " [0.9954137  0.00458626]\n",
            " [0.4884079  0.51159215]\n",
            " [0.80290586 0.19709408]\n",
            " [0.20717745 0.79282254]\n",
            " [0.2870471  0.7129529 ]\n",
            " [0.24856968 0.75143033]\n",
            " [0.23142245 0.76857764]\n",
            " [0.8596433  0.14035675]\n",
            " [0.1159068  0.8840932 ]\n",
            " [0.34194183 0.65805817]\n",
            " [0.08576841 0.9142316 ]\n",
            " [0.87530947 0.12469051]\n",
            " [0.02281943 0.9771806 ]\n",
            " [0.98597634 0.01402359]\n",
            " [0.15442717 0.8455729 ]\n",
            " [0.1027225  0.89727753]\n",
            " [0.04042723 0.95957273]\n",
            " [0.1163048  0.8836952 ]\n",
            " [0.00262984 0.9973701 ]\n",
            " [0.5743496  0.4256504 ]\n",
            " [0.4637746  0.53622544]\n",
            " [0.09564441 0.9043556 ]\n",
            " [0.526414   0.47358602]\n",
            " [0.5504423  0.4495577 ]\n",
            " [0.6611329  0.33886713]\n",
            " [0.29716718 0.70283276]\n",
            " [0.93761206 0.06238794]\n",
            " [0.00625145 0.99374855]\n",
            " [0.17094454 0.8290555 ]\n",
            " [0.9216489  0.07835104]\n",
            " [0.53153306 0.46846697]\n",
            " [0.62467724 0.37532276]\n",
            " [0.06014637 0.9398536 ]\n",
            " [0.30954158 0.6904584 ]\n",
            " [0.21088563 0.7891144 ]\n",
            " [0.7520122  0.24798784]\n",
            " [0.8078948  0.19210516]\n",
            " [0.15156406 0.8484359 ]\n",
            " [0.9002497  0.09975025]\n",
            " [0.25584093 0.7441591 ]\n",
            " [0.7394442  0.26055574]\n",
            " [0.06556018 0.9344398 ]\n",
            " [0.71102107 0.2889789 ]\n",
            " [0.09232388 0.9076761 ]\n",
            " [0.9865671  0.01343298]\n",
            " [0.5525551  0.44744498]\n",
            " [0.6050148  0.3949851 ]\n",
            " [0.4025433  0.5974567 ]\n",
            " [0.23476471 0.7652353 ]\n",
            " [0.7330847  0.26691532]\n",
            " [0.2759156  0.72408444]\n",
            " [0.8644674  0.1355326 ]\n",
            " [0.20596394 0.79403603]\n",
            " [0.194339   0.80566096]\n",
            " [0.5481525  0.45184746]\n",
            " [0.96091694 0.03908302]\n",
            " [0.67128503 0.32871497]\n",
            " [0.29046693 0.7095331 ]\n",
            " [0.05548996 0.94451   ]\n",
            " [0.9360086  0.06399146]\n",
            " [0.9002219  0.09977809]\n",
            " [0.9356443  0.06435568]\n",
            " [0.3842326  0.6157674 ]\n",
            " [0.37707666 0.62292343]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(figsize = (8,6),\n",
        "                        nrows = 2,\n",
        "                        )\n",
        "ax = axes.flatten()[0]\n",
        "ax.hist(y_test[:,-1],label = 'being non-living')\n",
        "ax.legend()\n",
        "ax = axes.flatten()[1]\n",
        "ax.hist(y_pred[:,-1],label = 'prob(non-living)')\n",
        "ax.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "bQ82HvCmgBWs",
        "outputId": "ca122f56-c685-4339-979d-ca7022c2a0d8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5949435b10>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ltdDAj44gcMC"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}