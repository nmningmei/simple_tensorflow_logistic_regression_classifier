{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple logistic regression implemented in tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBrcpY9CTIy1Fax+bmxsox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmningmei/simple_tensorflow_logistic_regression_classifier/blob/main/simple_logistic_regression_implemented_in_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr-BTjcucr8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/simple_tensorflow_logistic_regression_classifier\"):\n",
        "    !git clone https://github.com/nmningmei/simple_tensorflow_logistic_regression_classifier.git\n",
        "\n",
        "os.chdir(\"/content/simple_tensorflow_logistic_regression_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFgOMeqofD0X",
        "outputId": "223e993a-ba22-4751-fc02-48d2ae7c7eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  README.md  temp.h5  test.py  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from utils import (build_logistic_regression,\n",
        "                   compile_logistic_regression)\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "sYY0s-U4e5Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# experiment control"
      ],
      "metadata": {
        "id": "MCOodYUBft04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = int(1e3) # just a large number\n",
        "print_train = True"
      ],
      "metadata": {
        "id": "YZndB6f3e59i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clear memory states"
      ],
      "metadata": {
        "id": "NdG4FfQGfxAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Tw3hNm6ZffwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate random test data"
      ],
      "metadata": {
        "id": "0jB7pKhif0Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = make_classification(n_samples             = 150,\n",
        "                         n_features            = 100,\n",
        "                         n_informative         = 3,\n",
        "                         n_redundant           = 10,\n",
        "                         n_classes             = 2,\n",
        "                         n_clusters_per_class  = 4,\n",
        "                         flip_y                = .01,\n",
        "                         class_sep             = .5,# how easy to separate the two classes\n",
        "                         shuffle               = True,\n",
        "                         random_state          = 12345,\n",
        "                         )"
      ],
      "metadata": {
        "id": "DCFa9C5rfiBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# one-hot encoding for softmax"
      ],
      "metadata": {
        "id": "n2nC9Ppcf2Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.reshape((-1,1))\n",
        "y = np.hstack([y,1-y])"
      ],
      "metadata": {
        "id": "tAOp05CdfkTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split the data into train, validation, and test"
      ],
      "metadata": {
        "id": "ebMs-Eaff4Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test   = train_test_split(X,y,test_size = .1,random_state = 12345)\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size = .1,random_state = 12345)\n",
        "# add some 0.5 labeled data - don't use too much\n",
        "X_noise = np.random.normal(X_train.mean(),X_train.std(),size = (int(X_train.shape[0]/2),100))\n",
        "y_noise = np.array([[0.5,0.5]] * int(X_train.shape[0]/2))\n",
        "X_train = np.concatenate([X_train,X_noise])\n",
        "y_train = np.concatenate([y_train,y_noise])\n",
        "\n",
        "# X_noise = np.random.normal(X_test.mean(),X_test.std(),size = (int(X_test.shape[0]/2),100))\n",
        "# y_noise = np.array([[0.5,0.5]] * int(X_test.shape[0]/2))\n",
        "# X_test  = np.concatenate([X_test,X_noise])\n",
        "# y_test  = np.concatenate([y_test,y_noise])"
      ],
      "metadata": {
        "id": "kq47iHuYflgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build the model"
      ],
      "metadata": {
        "id": "qQTLEM33f6dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(12345)\n",
        "logistic_regression = build_logistic_regression(\n",
        "                        input_size              = X_train.shape[1],\n",
        "                        output_size             = 2,\n",
        "                        special                 = False,\n",
        "                        kernel_regularizer      = regularizers.L2(l2 = 1e-2),\n",
        "                        activity_regularizer    = regularizers.L1(l1 = 1e-32), # this makes the prediction sparse\n",
        "                        print_model             = True,\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiJ6x_QvfoGk",
        "outputId": "b031b8a6-0d73-4b16-d068-91a4e38398c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"logistic_regression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 100)]             0         \n",
            "                                                                 \n",
            " logistic_layer (Dense)      (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 202\n",
            "Trainable params: 202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compile the model"
      ],
      "metadata": {
        "id": "3U7ZrMh-f8Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression,callbacks = compile_logistic_regression(\n",
        "                                logistic_regression,\n",
        "                                model_name      = 'temp.h5',\n",
        "                                optimizer       = None,\n",
        "                                loss_function   = None,\n",
        "                                metric          = None,\n",
        "                                callbacks       = None,\n",
        "                                learning_rate   = 1e-3,\n",
        "                                tol             = 1e-4,\n",
        "                                patience        = 10,\n",
        "                                )"
      ],
      "metadata": {
        "id": "IPgnTMRGfpW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train and validate the model"
      ],
      "metadata": {
        "id": "5rskK3z9f9pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression.fit(\n",
        "                        X_train,\n",
        "                        y_train,\n",
        "                        batch_size      = 4,\n",
        "                        epochs          = n_epochs,\n",
        "                        verbose         = print_train,\n",
        "                        callbacks       = callbacks,\n",
        "                        validation_data = (X_valid,y_valid),\n",
        "                        shuffle         = True,\n",
        "                        class_weight    = {0:1,1:2},# tf has this but I don't think it is the same as sklearn\n",
        "                        )\n",
        "y_pred = logistic_regression.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57HViFjhfqmg",
        "outputId": "4cdd659c-703f-4b46-8948-643f6bc822c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "46/46 [==============================] - 1s 8ms/step - loss: 1.2658 - auc: 0.4931 - val_loss: 1.0810 - val_auc: 0.4490\n",
            "Epoch 2/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2471 - auc: 0.5016 - val_loss: 1.0710 - val_auc: 0.4439\n",
            "Epoch 3/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2291 - auc: 0.5102 - val_loss: 1.0615 - val_auc: 0.4388\n",
            "Epoch 4/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.2119 - auc: 0.5175 - val_loss: 1.0523 - val_auc: 0.4337\n",
            "Epoch 5/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1950 - auc: 0.5268 - val_loss: 1.0436 - val_auc: 0.4388\n",
            "Epoch 6/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1790 - auc: 0.5356 - val_loss: 1.0362 - val_auc: 0.4388\n",
            "Epoch 7/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1630 - auc: 0.5432 - val_loss: 1.0281 - val_auc: 0.4388\n",
            "Epoch 8/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1480 - auc: 0.5508 - val_loss: 1.0210 - val_auc: 0.4388\n",
            "Epoch 9/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1336 - auc: 0.5592 - val_loss: 1.0141 - val_auc: 0.4337\n",
            "Epoch 10/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1188 - auc: 0.5672 - val_loss: 1.0078 - val_auc: 0.4388\n",
            "Epoch 11/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1058 - auc: 0.5748 - val_loss: 1.0009 - val_auc: 0.4286\n",
            "Epoch 12/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0930 - auc: 0.5812 - val_loss: 0.9954 - val_auc: 0.4388\n",
            "Epoch 13/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0808 - auc: 0.5893 - val_loss: 0.9894 - val_auc: 0.4388\n",
            "Epoch 14/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0688 - auc: 0.5976 - val_loss: 0.9835 - val_auc: 0.4490\n",
            "Epoch 15/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0574 - auc: 0.6049 - val_loss: 0.9771 - val_auc: 0.4541\n",
            "Epoch 16/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0461 - auc: 0.6144 - val_loss: 0.9719 - val_auc: 0.4592\n",
            "Epoch 17/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0356 - auc: 0.6229 - val_loss: 0.9671 - val_auc: 0.4592\n",
            "Epoch 18/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0254 - auc: 0.6299 - val_loss: 0.9630 - val_auc: 0.4592\n",
            "Epoch 19/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0155 - auc: 0.6367 - val_loss: 0.9586 - val_auc: 0.4592\n",
            "Epoch 20/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0058 - auc: 0.6446 - val_loss: 0.9543 - val_auc: 0.4592\n",
            "Epoch 21/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9963 - auc: 0.6521 - val_loss: 0.9508 - val_auc: 0.4592\n",
            "Epoch 22/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9876 - auc: 0.6573 - val_loss: 0.9468 - val_auc: 0.4541\n",
            "Epoch 23/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9791 - auc: 0.6626 - val_loss: 0.9431 - val_auc: 0.4541\n",
            "Epoch 24/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9710 - auc: 0.6684 - val_loss: 0.9395 - val_auc: 0.4464\n",
            "Epoch 25/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9629 - auc: 0.6744 - val_loss: 0.9361 - val_auc: 0.4388\n",
            "Epoch 26/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9552 - auc: 0.6790 - val_loss: 0.9320 - val_auc: 0.4286\n",
            "Epoch 27/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9474 - auc: 0.6842 - val_loss: 0.9295 - val_auc: 0.4286\n",
            "Epoch 28/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9401 - auc: 0.6893 - val_loss: 0.9264 - val_auc: 0.4260\n",
            "Epoch 29/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.9330 - auc: 0.6940 - val_loss: 0.9235 - val_auc: 0.4388\n",
            "Epoch 30/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9262 - auc: 0.6971 - val_loss: 0.9198 - val_auc: 0.4388\n",
            "Epoch 31/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9193 - auc: 0.7016 - val_loss: 0.9172 - val_auc: 0.4388\n",
            "Epoch 32/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9129 - auc: 0.7051 - val_loss: 0.9140 - val_auc: 0.4388\n",
            "Epoch 33/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.9066 - auc: 0.7079 - val_loss: 0.9116 - val_auc: 0.4388\n",
            "Epoch 34/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.9005 - auc: 0.7113 - val_loss: 0.9095 - val_auc: 0.4388\n",
            "Epoch 35/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8947 - auc: 0.7148 - val_loss: 0.9077 - val_auc: 0.4388\n",
            "Epoch 36/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8890 - auc: 0.7176 - val_loss: 0.9057 - val_auc: 0.4490\n",
            "Epoch 37/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8835 - auc: 0.7210 - val_loss: 0.9031 - val_auc: 0.4490\n",
            "Epoch 38/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8782 - auc: 0.7237 - val_loss: 0.9011 - val_auc: 0.4541\n",
            "Epoch 39/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8730 - auc: 0.7265 - val_loss: 0.8994 - val_auc: 0.4541\n",
            "Epoch 40/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8679 - auc: 0.7290 - val_loss: 0.8983 - val_auc: 0.4592\n",
            "Epoch 41/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8631 - auc: 0.7315 - val_loss: 0.8969 - val_auc: 0.4592\n",
            "Epoch 42/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8583 - auc: 0.7351 - val_loss: 0.8954 - val_auc: 0.4592\n",
            "Epoch 43/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8536 - auc: 0.7382 - val_loss: 0.8940 - val_auc: 0.4592\n",
            "Epoch 44/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8491 - auc: 0.7415 - val_loss: 0.8930 - val_auc: 0.4643\n",
            "Epoch 45/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8446 - auc: 0.7450 - val_loss: 0.8918 - val_auc: 0.4694\n",
            "Epoch 46/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8404 - auc: 0.7471 - val_loss: 0.8921 - val_auc: 0.4694\n",
            "Epoch 47/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8362 - auc: 0.7500 - val_loss: 0.8903 - val_auc: 0.4745\n",
            "Epoch 48/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8321 - auc: 0.7525 - val_loss: 0.8899 - val_auc: 0.4745\n",
            "Epoch 49/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8282 - auc: 0.7553 - val_loss: 0.8896 - val_auc: 0.4694\n",
            "Epoch 50/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8244 - auc: 0.7576 - val_loss: 0.8888 - val_auc: 0.4719\n",
            "Epoch 51/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8204 - auc: 0.7591 - val_loss: 0.8878 - val_auc: 0.4719\n",
            "Epoch 52/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8168 - auc: 0.7616 - val_loss: 0.8873 - val_auc: 0.4745\n",
            "Epoch 53/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8131 - auc: 0.7638 - val_loss: 0.8864 - val_auc: 0.4643\n",
            "Epoch 54/1000\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.8097 - auc: 0.7653 - val_loss: 0.8861 - val_auc: 0.4694\n",
            "Epoch 55/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8064 - auc: 0.7678 - val_loss: 0.8861 - val_auc: 0.4694\n",
            "Epoch 56/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.8031 - auc: 0.7703 - val_loss: 0.8858 - val_auc: 0.4694\n",
            "Epoch 57/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7998 - auc: 0.7732 - val_loss: 0.8864 - val_auc: 0.4694\n",
            "Epoch 58/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7969 - auc: 0.7740 - val_loss: 0.8852 - val_auc: 0.4694\n",
            "Epoch 59/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7937 - auc: 0.7767 - val_loss: 0.8845 - val_auc: 0.4796\n",
            "Epoch 60/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7906 - auc: 0.7776 - val_loss: 0.8841 - val_auc: 0.4796\n",
            "Epoch 61/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7877 - auc: 0.7795 - val_loss: 0.8837 - val_auc: 0.4847\n",
            "Epoch 62/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7847 - auc: 0.7811 - val_loss: 0.8832 - val_auc: 0.4847\n",
            "Epoch 63/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7820 - auc: 0.7828 - val_loss: 0.8839 - val_auc: 0.4847\n",
            "Epoch 64/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7792 - auc: 0.7847 - val_loss: 0.8833 - val_auc: 0.4796\n",
            "Epoch 65/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7765 - auc: 0.7858 - val_loss: 0.8824 - val_auc: 0.4745\n",
            "Epoch 66/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7738 - auc: 0.7877 - val_loss: 0.8823 - val_auc: 0.4745\n",
            "Epoch 67/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7711 - auc: 0.7890 - val_loss: 0.8829 - val_auc: 0.4745\n",
            "Epoch 68/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7685 - auc: 0.7909 - val_loss: 0.8826 - val_auc: 0.4745\n",
            "Epoch 69/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7661 - auc: 0.7925 - val_loss: 0.8822 - val_auc: 0.4745\n",
            "Epoch 70/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7637 - auc: 0.7943 - val_loss: 0.8820 - val_auc: 0.4745\n",
            "Epoch 71/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7613 - auc: 0.7958 - val_loss: 0.8817 - val_auc: 0.4745\n",
            "Epoch 72/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7590 - auc: 0.7973 - val_loss: 0.8809 - val_auc: 0.4745\n",
            "Epoch 73/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7567 - auc: 0.7978 - val_loss: 0.8807 - val_auc: 0.4745\n",
            "Epoch 74/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7545 - auc: 0.7997 - val_loss: 0.8807 - val_auc: 0.4796\n",
            "Epoch 75/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7521 - auc: 0.8012 - val_loss: 0.8805 - val_auc: 0.4796\n",
            "Epoch 76/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7501 - auc: 0.8026 - val_loss: 0.8803 - val_auc: 0.4694\n",
            "Epoch 77/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7480 - auc: 0.8040 - val_loss: 0.8796 - val_auc: 0.4745\n",
            "Epoch 78/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7459 - auc: 0.8045 - val_loss: 0.8790 - val_auc: 0.4694\n",
            "Epoch 79/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7436 - auc: 0.8057 - val_loss: 0.8795 - val_auc: 0.4694\n",
            "Epoch 80/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7416 - auc: 0.8063 - val_loss: 0.8797 - val_auc: 0.4643\n",
            "Epoch 81/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7397 - auc: 0.8074 - val_loss: 0.8795 - val_auc: 0.4643\n",
            "Epoch 82/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7378 - auc: 0.8086 - val_loss: 0.8792 - val_auc: 0.4643\n",
            "Epoch 83/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7358 - auc: 0.8092 - val_loss: 0.8791 - val_auc: 0.4643\n",
            "Epoch 84/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7340 - auc: 0.8105 - val_loss: 0.8787 - val_auc: 0.4643\n",
            "Epoch 85/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7322 - auc: 0.8114 - val_loss: 0.8786 - val_auc: 0.4643\n",
            "Epoch 86/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7304 - auc: 0.8129 - val_loss: 0.8784 - val_auc: 0.4643\n",
            "Epoch 87/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7286 - auc: 0.8136 - val_loss: 0.8779 - val_auc: 0.4643\n",
            "Epoch 88/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7269 - auc: 0.8145 - val_loss: 0.8796 - val_auc: 0.4643\n",
            "Epoch 89/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7253 - auc: 0.8158 - val_loss: 0.8797 - val_auc: 0.4643\n",
            "Epoch 90/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7237 - auc: 0.8167 - val_loss: 0.8797 - val_auc: 0.4643\n",
            "Epoch 91/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7220 - auc: 0.8174 - val_loss: 0.8788 - val_auc: 0.4694\n",
            "Epoch 92/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7205 - auc: 0.8191 - val_loss: 0.8798 - val_auc: 0.4745\n",
            "Epoch 93/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7189 - auc: 0.8196 - val_loss: 0.8798 - val_auc: 0.4745\n",
            "Epoch 94/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7174 - auc: 0.8205 - val_loss: 0.8799 - val_auc: 0.4694\n",
            "Epoch 95/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7159 - auc: 0.8215 - val_loss: 0.8798 - val_auc: 0.4694\n",
            "Epoch 96/1000\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.7144 - auc: 0.8216 - val_loss: 0.8800 - val_auc: 0.4745\n",
            "Epoch 97/1000\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7130 - auc: 0.8225 - val_loss: 0.8801 - val_auc: 0.4694\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f242cc23dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'test score = {roc_auc_score(y_test,y_pred,):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fs-lPu4ge2G",
        "outputId": "e7fd56c8-3241-4f72-8797-6c0d5be3ca55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test score = 0.6481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualize the distribution of the predictions"
      ],
      "metadata": {
        "id": "-xkyrZVbf_cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5ACRnEGhW-z",
        "outputId": "a291d865-6908-4438-f273-ac139cc311a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]]\n",
            "[[0.5717424  0.42825764]\n",
            " [0.25801748 0.7419825 ]\n",
            " [0.01492926 0.9850707 ]\n",
            " [0.968661   0.03133904]\n",
            " [0.2590448  0.7409551 ]\n",
            " [0.6795     0.3205    ]\n",
            " [0.79624325 0.20375675]\n",
            " [0.60854    0.39145997]\n",
            " [0.95375675 0.04624325]\n",
            " [0.24453911 0.7554609 ]\n",
            " [0.9715366  0.02846345]\n",
            " [0.40048546 0.59951454]\n",
            " [0.12197227 0.8780278 ]\n",
            " [0.937617   0.062383  ]\n",
            " [0.7587101  0.24128997]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(figsize = (8,6),\n",
        "                        nrows = 2,\n",
        "                        )\n",
        "ax = axes.flatten()[0]\n",
        "ax.hist(y_test[:,-1],label = 'being non-living')\n",
        "ax.legend()\n",
        "ax = axes.flatten()[1]\n",
        "ax.hist(y_pred[:,-1],label = 'prob(non-living)')\n",
        "ax.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "bQ82HvCmgBWs",
        "outputId": "016fe5d0-7da8-4ea3-f31a-e1e09b36b5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2430e65350>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ltdDAj44gcMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}