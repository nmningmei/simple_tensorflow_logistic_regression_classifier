{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "customized_training_with_noise_instances (fMRI data example).ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQNNseTs24AgPl6iuygeNm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmningmei/simple_tensorflow_logistic_regression_classifier/blob/main/customized_training_with_noise_instances_(fMRI_data_example).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This script adapts from [Customizing what happens in fit()](https://keras.io/guides/customizing_what_happens_in_fit/) for particular use in our lab"
      ],
      "metadata": {
        "id": "90PQh2trNsO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu_OPvIL-ihK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/simple_tensorflow_logistic_regression_classifier\"):\n",
        "    !git clone https://github.com/nmningmei/simple_tensorflow_logistic_regression_classifier.git\n",
        "\n",
        "os.chdir(\"/content/simple_tensorflow_logistic_regression_classifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload the data"
      ],
      "metadata": {
        "id": "dKGcC-GF8MZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "8qHcLvpi8Ob4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_dictionary = {'test.csv':'1ZNryNfmV3S3W66JfFo9n-NWfazVz2idW',\n",
        "                   'test.npy':'1Uytrqj8biQGq-w0VbFJxm7xOH52Cra-Z',\n",
        "                   'train.csv':'1C9JFuZkmcRAan56LBrDYtLdfq_iPRO7j',\n",
        "                   'train.npy':'1ra9oL-9Mdy6elcnW6daeK0esGHeyGfe0'}\n",
        "for file_name,file_id in file_dictionary.items():\n",
        "    downloaded = drive.CreateFile({'id':file_id})\n",
        "    downloaded.GetContentFile(file_name)"
      ],
      "metadata": {
        "id": "_ieJqhwY-RT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0g-eE3J-qzq",
        "outputId": "2b8bb9c9-cd65-4998-e015-a14f99d50156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customized_training_with_noise_instances.ipynb              temp.h5   train.csv\n",
            "LICENSE                                                     test.csv  train.npy\n",
            "README.md                                                   test.npy  utils.py\n",
            "simple_logistic_regression_implemented_in_tensorflow.ipynb  test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,models,initializers,optimizers,losses,metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "l7TMeW_6-tle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# experiment control"
      ],
      "metadata": {
        "id": "ZaWNuc3R-wuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = int(1e3) # just a large number\n",
        "print_train = True\n",
        "batch_size = 8\n",
        "n_noise = 1 # number of noise inputs per epoch\n",
        "learning_rate = 1e-2\n",
        "tol = 1e-4\n",
        "patience = 10"
      ],
      "metadata": {
        "id": "zeuIAmCp-v-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clear memory states"
      ],
      "metadata": {
        "id": "lmtnWNhg-6D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "nrLCWDFK-375"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0: nonliving, 1: living, 2: noise"
      ],
      "metadata": {
        "id": "IPY8HFANA5rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(exclude = None):\n",
        "    X_train = np.load('train.npy')\n",
        "    df_train = pd.read_csv('train.csv')\n",
        "    y_train = df_train['target_category'].values\n",
        "    X_test = np.load('test.npy')\n",
        "    df_test = pd.read_csv('test.csv')\n",
        "    y_test = df_test['target_category'].values\n",
        "    if exclude is not None:\n",
        "        idx_train = y_train != exclude\n",
        "\n",
        "        X_train = X_train[idx_train]\n",
        "        y_train = y_train[idx_train]\n",
        "    return X_train,y_train,X_test,y_test\n",
        "X_train,y_train,X_test,y_test = load_data(exclude = 2)"
      ],
      "metadata": {
        "id": "Im-3AE2O-8Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding for softmax"
      ],
      "metadata": {
        "id": "UfxkkzQz_HnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(y):\n",
        "    y = np.array(y == 0,dtype = 'float32')\n",
        "    y = y.reshape((-1,1))\n",
        "    y = np.hstack([y,1-y]).astype('float32')\n",
        "    return y\n",
        "y_train = one_hot_encoding(y_train)\n",
        "y_test = one_hot_encoding(y_test)"
      ],
      "metadata": {
        "id": "uVHDJUB8_Evd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split the data into train, validation, and test"
      ],
      "metadata": {
        "id": "9spg-dko_ON0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size = .1,random_state = 12345)"
      ],
      "metadata": {
        "id": "Le8tMmtE_J94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build the model"
      ],
      "metadata": {
        "id": "vWPRmh7L_YlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "m2T8kv0CFwRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class simple_logistic_regression(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        This function is used during `fit()`\n",
        "\n",
        "        We manually add noise instances in each batch of training to increase\n",
        "        the diversity of the noise that the model sees.\n",
        "        \"\"\"\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "        x_mean,x_std = tf.nn.moments(x,axes = 0)\n",
        "        with tf.GradientTape() as tape:\n",
        "            x_noise = tf.random.normal(shape = (n_noise,x.shape[1]),\n",
        "                                       mean = x_mean,\n",
        "                                       stddev = x_std,\n",
        "                                       name = 'x_noise')\n",
        "            y_noise = tf.constant([[0.5,0.5]] * n_noise,dtype = \"float32\")\n",
        "            x_train = tf.concat([x,x_noise],0)\n",
        "            y_train = tf.concat([y,y_noise],0)\n",
        "            idx = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(idx)\n",
        "            x_train = tf.gather(x_train,idx,axis = 0)\n",
        "            y_train = tf.gather(y_train,idx,axis = 0)\n",
        "            y_pred = self(x_train, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y_train, y_pred, \n",
        "                                      regularization_losses = self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y_train, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    def test_step(self, data):\n",
        "        \"\"\"\n",
        "        We must have the testing function to avoid double dipping in testing\n",
        "        \"\"\"\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self(x, training=False)\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "def build_model(input_size,output_size = 2,kernel_regularizer = None,activity_regularizer = None):\n",
        "    \"\"\"\n",
        "    This function builds the logistic regression classifier using the customized\n",
        "    modeling building method we define above\n",
        "\n",
        "    Inputs\n",
        "    ---\n",
        "    input_size: int, the 2nd dimension of the input features\n",
        "    output_size: int, default = 2\n",
        "    kernel_regularizer: None or keras.regularizers\n",
        "    activity_regularizer: None or keras.regularizers\n",
        "\n",
        "    Outputs\n",
        "    ---\n",
        "    model: keras.Models\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(12345)\n",
        "    input_layer = layers.Input(shape        = (input_size,),\n",
        "                                name         = \"input_layer\",)\n",
        "\n",
        "    logistic_layer = layers.Dense(units                 = output_size,\n",
        "                                  activation            = 'softmax',\n",
        "                                  use_bias              = True,\n",
        "                                  kernel_initializer    = initializers.HeNormal(),\n",
        "                                  kernel_regularizer    = kernel_regularizer,\n",
        "                                  activity_regularizer  = activity_regularizer,\n",
        "                                  name                  = 'logistic_layer'\n",
        "                                  )(input_layer)\n",
        "    model = simple_logistic_regression(input_layer,logistic_layer,\n",
        "                                       name = 'logistic_regression')\n",
        "    return model\n",
        "# the most important helper function: early stopping and model saving\n",
        "def make_CallBackList(model_name,monitor='val_loss',mode='min',verbose=0,min_delta=1e-4,patience=50,frequency = 1):\n",
        "    \n",
        "    \"\"\"\n",
        "    Make call back function lists for the keras models\n",
        "    \n",
        "    Parameters\n",
        "    -------------------------\n",
        "    model_name : str,\n",
        "        directory of where we want to save the model and its name\n",
        "    monitor : str, default = 'val_loss'\n",
        "        the criterion we used for saving or stopping the model\n",
        "    mode : str, default = 'min'\n",
        "        min --> lower the better, max --> higher the better\n",
        "    verboser : int or bool, default = 0\n",
        "        printout the monitoring messages\n",
        "    min_delta : float, default = 1e-4\n",
        "        minimum change for early stopping\n",
        "    patience : int, default = 50\n",
        "        temporal windows of the minimum change monitoring\n",
        "    frequency : int, default = 1\n",
        "        temporal window steps of the minimum change monitoring\n",
        "    \n",
        "    Return\n",
        "    --------------------------\n",
        "    CheckPoint : tensorflow.keras.callbacks\n",
        "        saving the best model\n",
        "    EarlyStopping : tensorflow.keras.callbacks\n",
        "        early stoppi\n",
        "    \"\"\"\n",
        "    checkPoint = ModelCheckpoint(model_name,# saving path\n",
        "                                 monitor          = monitor,# saving criterion\n",
        "                                 save_best_only   = True,# save only the best model\n",
        "                                 mode             = mode,# saving criterion\n",
        "                                 verbose          = verbose,# print out (>1) or not (0)\n",
        "                                 )\n",
        "    earlyStop = EarlyStopping(   monitor          = monitor,\n",
        "                                 min_delta        = min_delta,\n",
        "                                 patience         = patience,\n",
        "                                 verbose          = verbose, \n",
        "                                 mode             = mode,\n",
        "                                 )\n",
        "    return [checkPoint,earlyStop]\n",
        "def compile_logistic_regression(\n",
        "                    model,\n",
        "                    model_name      = 'temp.h5',\n",
        "                    optimizer       = None,\n",
        "                    loss_function   = None,\n",
        "                    metric          = None,\n",
        "                    callbacks       = None,\n",
        "                    learning_rate   = 1e-2,\n",
        "                    tol             = 1e-4,\n",
        "                    patience        = 5,\n",
        "                    ):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "    ---\n",
        "    model: tf.keras.models.Model or callable tf objects\n",
        "    model_name: str, directory of where we want to save the model and its name\n",
        "    optimizer: None or tf.keras.optimizers, default = SGD\n",
        "    loss_function: None or tf.keras.losses, default = BinaryCrossentropy\n",
        "    metric: None or tf.keras.metrics, default = AUC\n",
        "    callbacks: None or list of tf.keras.callbacks, default = [checkpoint,earlystopping]\n",
        "    learning_rate: float, learning rate, default = 1e-2,\n",
        "    tol: float, for determining when to stop training, default = 1e-4,\n",
        "    patience: int, for determing when to stop training, default = 5,\n",
        "    \n",
        "    Outputs\n",
        "    ---\n",
        "    model: tf.keras.models.Model or callable tf objects\n",
        "    callbacks:ist of tf.keras.callbacks\n",
        "    \"\"\"\n",
        "    if optimizer is None:\n",
        "        optimizer       = optimizers.SGD(learning_rate = learning_rate,)\n",
        "    if loss_function is None:\n",
        "        loss_function   = losses.BinaryCrossentropy()\n",
        "    if metric is None:\n",
        "        metric          = metrics.AUC()\n",
        "    if callbacks is None:\n",
        "        callbacks       = make_CallBackList(\n",
        "                                      model_name    = model_name,\n",
        "                                      monitor       = 'val_loss',\n",
        "                                      mode          = 'min',\n",
        "                                      verbose       = 0,\n",
        "                                      min_delta     = tol,\n",
        "                                      patience      = patience,\n",
        "                                      frequency     = 1,\n",
        "                                      )\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss      = loss_function,\n",
        "                  metrics   = [metric],\n",
        "                  run_eagerly = True,\n",
        "                  )\n",
        "    return model,callbacks"
      ],
      "metadata": {
        "id": "rWZPDO8k_VRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the helper functions to build the models"
      ],
      "metadata": {
        "id": "qsCPR1FZGGUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model = build_model(input_size = X_train.shape[1],output_size = 2,)\n",
        "logistic_regression_model,callbacks = compile_logistic_regression(logistic_regression_model,\n",
        "                                                                  learning_rate = learning_rate,\n",
        "                                                                  tol = tol,\n",
        "                                                                  patience = patience,\n",
        "                                                                  )"
      ],
      "metadata": {
        "id": "oCVp-Wu3E0si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "cKG0hQGAL1Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model.fit(X_train,y_train,\n",
        "                              batch_size = batch_size,\n",
        "                              validation_data = (X_valid,y_valid),\n",
        "                              epochs = n_epochs,\n",
        "                              callbacks = callbacks,\n",
        "                              shuffle = True,\n",
        "                              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WNzUIHyEQHp",
        "outputId": "f9477ff7-f783-45a0-c580-d9784e84ed60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "53/53 [==============================] - 1s 26ms/step - loss: 0.4295 - auc: 0.9219 - val_loss: 0.2978 - val_auc: 0.9507\n",
            "Epoch 2/1000\n",
            "53/53 [==============================] - 1s 24ms/step - loss: 0.3195 - auc: 0.9664 - val_loss: 0.2757 - val_auc: 0.9647\n",
            "Epoch 3/1000\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 0.2242 - auc: 0.9801 - val_loss: 0.2553 - val_auc: 0.9638\n",
            "Epoch 4/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2272 - auc: 0.9824 - val_loss: 0.2939 - val_auc: 0.9570\n",
            "Epoch 5/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2552 - auc: 0.9830 - val_loss: 0.2500 - val_auc: 0.9606\n",
            "Epoch 6/1000\n",
            "53/53 [==============================] - 1s 18ms/step - loss: 0.2130 - auc: 0.9855 - val_loss: 0.2397 - val_auc: 0.9597\n",
            "Epoch 7/1000\n",
            "53/53 [==============================] - 1s 18ms/step - loss: 0.1925 - auc: 0.9843 - val_loss: 0.2363 - val_auc: 0.9602\n",
            "Epoch 8/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.1955 - auc: 0.9833 - val_loss: 0.2309 - val_auc: 0.9756\n",
            "Epoch 9/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2507 - auc: 0.9864 - val_loss: 0.3864 - val_auc: 0.9269\n",
            "Epoch 10/1000\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 0.2305 - auc: 0.9789 - val_loss: 0.2405 - val_auc: 0.9724\n",
            "Epoch 11/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.1996 - auc: 0.9855 - val_loss: 0.2643 - val_auc: 0.9683\n",
            "Epoch 12/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2299 - auc: 0.9872 - val_loss: 0.2303 - val_auc: 0.9742\n",
            "Epoch 13/1000\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 0.2177 - auc: 0.9824 - val_loss: 0.2322 - val_auc: 0.9742\n",
            "Epoch 14/1000\n",
            "53/53 [==============================] - 1s 20ms/step - loss: 0.1960 - auc: 0.9836 - val_loss: 0.2054 - val_auc: 0.9638\n",
            "Epoch 15/1000\n",
            "53/53 [==============================] - 1s 20ms/step - loss: 0.2343 - auc: 0.9794 - val_loss: 0.2322 - val_auc: 0.9737\n",
            "Epoch 16/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2248 - auc: 0.9873 - val_loss: 0.2188 - val_auc: 0.9588\n",
            "Epoch 17/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.1902 - auc: 0.9870 - val_loss: 0.2285 - val_auc: 0.9574\n",
            "Epoch 18/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.1908 - auc: 0.9866 - val_loss: 0.2784 - val_auc: 0.9348\n",
            "Epoch 19/1000\n",
            "53/53 [==============================] - 1s 23ms/step - loss: 0.2412 - auc: 0.9842 - val_loss: 0.2437 - val_auc: 0.9559\n",
            "Epoch 20/1000\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 0.2067 - auc: 0.9883 - val_loss: 0.2993 - val_auc: 0.9357\n",
            "Epoch 21/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2236 - auc: 0.9879 - val_loss: 0.2275 - val_auc: 0.9615\n",
            "Epoch 22/1000\n",
            "53/53 [==============================] - 1s 21ms/step - loss: 0.2298 - auc: 0.9881 - val_loss: 0.2695 - val_auc: 0.9384\n",
            "Epoch 23/1000\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 0.2230 - auc: 0.9832 - val_loss: 0.2315 - val_auc: 0.9638\n",
            "Epoch 24/1000\n",
            "53/53 [==============================] - 1s 22ms/step - loss: 0.1989 - auc: 0.9797 - val_loss: 0.2600 - val_auc: 0.9212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f34bd8e1610>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "Kc-7OSJTLmpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict on random data"
      ],
      "metadata": {
        "id": "nvUTYuM9MGxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train,X_test,y_test = load_data(exclude = None)\n",
        "X_noise = X_test.copy()"
      ],
      "metadata": {
        "id": "-Xgefw91L8kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_noise_pred = logistic_regression_model.predict(X_noise)"
      ],
      "metadata": {
        "id": "Wd7b_D9ZMZex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_noise_pred[:,-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "tvBi4cENMjhu",
        "outputId": "438223a2-8851-455a-e318-78bf7e209c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([479.,  86.,  53.,  43.,  43.,  45.,  57.,  45.,  78., 476.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],\n",
              "       dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train the model without using the Gaussian noise samples"
      ],
      "metadata": {
        "id": "0dOV3GViFEm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train,X_test,y_test = load_data(exclude = None)\n",
        "label_maps = {0:[1,0],\n",
        "              1:[0,1],\n",
        "              2:[0.5,0.5]}\n",
        "y_train = np.array([label_maps[item] for item in y_train])\n",
        "y_test = np.array([label_maps[item] for item in y_test])\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size = .1,random_state = 12345)\n",
        "def build_model(input_size,output_size = 2,kernel_regularizer = None,activity_regularizer = None):\n",
        "    \"\"\"\n",
        "    This function builds the logistic regression classifier using the customized\n",
        "    modeling building method we define above\n",
        "\n",
        "    Inputs\n",
        "    ---\n",
        "    input_size: int, the 2nd dimension of the input features\n",
        "    output_size: int, default = 2\n",
        "    kernel_regularizer: None or keras.regularizers\n",
        "    activity_regularizer: None or keras.regularizers\n",
        "\n",
        "    Outputs\n",
        "    ---\n",
        "    model: keras.Models\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(12345)\n",
        "    input_layer = layers.Input(shape        = (input_size,),\n",
        "                                name         = \"input_layer\",)\n",
        "\n",
        "    logistic_layer = layers.Dense(units                 = output_size,\n",
        "                                  activation            = 'softmax',\n",
        "                                  use_bias              = True,\n",
        "                                  kernel_initializer    = initializers.HeNormal(),\n",
        "                                  kernel_regularizer    = kernel_regularizer,\n",
        "                                  activity_regularizer  = activity_regularizer,\n",
        "                                  name                  = 'logistic_layer'\n",
        "                                  )(input_layer)\n",
        "    model = models.Model(input_layer,logistic_layer,name = 'logistic_regression')\n",
        "    return model\n",
        "# the most important helper function: early stopping and model saving\n",
        "def make_CallBackList(model_name,monitor='val_loss',mode='min',verbose=0,min_delta=1e-4,patience=50,frequency = 1):\n",
        "    \n",
        "    \"\"\"\n",
        "    Make call back function lists for the keras models\n",
        "    \n",
        "    Parameters\n",
        "    -------------------------\n",
        "    model_name : str,\n",
        "        directory of where we want to save the model and its name\n",
        "    monitor : str, default = 'val_loss'\n",
        "        the criterion we used for saving or stopping the model\n",
        "    mode : str, default = 'min'\n",
        "        min --> lower the better, max --> higher the better\n",
        "    verboser : int or bool, default = 0\n",
        "        printout the monitoring messages\n",
        "    min_delta : float, default = 1e-4\n",
        "        minimum change for early stopping\n",
        "    patience : int, default = 50\n",
        "        temporal windows of the minimum change monitoring\n",
        "    frequency : int, default = 1\n",
        "        temporal window steps of the minimum change monitoring\n",
        "    \n",
        "    Return\n",
        "    --------------------------\n",
        "    CheckPoint : tensorflow.keras.callbacks\n",
        "        saving the best model\n",
        "    EarlyStopping : tensorflow.keras.callbacks\n",
        "        early stoppi\n",
        "    \"\"\"\n",
        "    checkPoint = ModelCheckpoint(model_name,# saving path\n",
        "                                 monitor          = monitor,# saving criterion\n",
        "                                 save_best_only   = True,# save only the best model\n",
        "                                 mode             = mode,# saving criterion\n",
        "                                 verbose          = verbose,# print out (>1) or not (0)\n",
        "                                 )\n",
        "    earlyStop = EarlyStopping(   monitor          = monitor,\n",
        "                                 min_delta        = min_delta,\n",
        "                                 patience         = patience,\n",
        "                                 verbose          = verbose, \n",
        "                                 mode             = mode,\n",
        "                                 )\n",
        "    return [checkPoint,earlyStop]\n",
        "logistic_regression_model = build_model(input_size = X_train.shape[1],output_size = 2,)\n",
        "logistic_regression_model,callbacks = compile_logistic_regression(logistic_regression_model,\n",
        "                                                                  learning_rate = learning_rate,\n",
        "                                                                  tol = tol,\n",
        "                                                                  patience = patience,\n",
        "                                                                  )\n",
        "logistic_regression_model.fit(X_train,y_train,\n",
        "                              batch_size = batch_size,\n",
        "                              validation_data = (X_valid,y_valid),\n",
        "                              epochs = n_epochs,\n",
        "                              callbacks = callbacks,\n",
        "                              shuffle = True,\n",
        "                              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL3k0S2uFJ-h",
        "outputId": "38d4f520-7ddd-445d-d988-a2d67fd8499e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 0.6566 - auc_1: 0.8424 - val_loss: 0.8525 - val_auc_1: 0.8048\n",
            "Epoch 2/1000\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 0.4498 - auc_1: 0.9382 - val_loss: 0.4692 - val_auc_1: 0.9546\n",
            "Epoch 3/1000\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 0.3391 - auc_1: 0.9773 - val_loss: 0.4321 - val_auc_1: 0.9746\n",
            "Epoch 4/1000\n",
            "79/79 [==============================] - 1s 19ms/step - loss: 0.3069 - auc_1: 0.9897 - val_loss: 0.4259 - val_auc_1: 0.9801\n",
            "Epoch 5/1000\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.2828 - auc_1: 0.9966 - val_loss: 0.4145 - val_auc_1: 0.9822\n",
            "Epoch 6/1000\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 0.2712 - auc_1: 0.9987 - val_loss: 0.4206 - val_auc_1: 0.9773\n",
            "Epoch 7/1000\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.2644 - auc_1: 0.9993 - val_loss: 0.4256 - val_auc_1: 0.9792\n",
            "Epoch 8/1000\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.2574 - auc_1: 0.9999 - val_loss: 0.4187 - val_auc_1: 0.9820\n",
            "Epoch 9/1000\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.2537 - auc_1: 1.0000 - val_loss: 0.4177 - val_auc_1: 0.9797\n",
            "Epoch 10/1000\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 0.2505 - auc_1: 1.0000 - val_loss: 0.4210 - val_auc_1: 0.9784\n",
            "Epoch 11/1000\n",
            "79/79 [==============================] - 2s 25ms/step - loss: 0.2484 - auc_1: 1.0000 - val_loss: 0.4200 - val_auc_1: 0.9801\n",
            "Epoch 12/1000\n",
            "79/79 [==============================] - 1s 16ms/step - loss: 0.2436 - auc_1: 1.0000 - val_loss: 0.4200 - val_auc_1: 0.9794\n",
            "Epoch 14/1000\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 0.2421 - auc_1: 1.0000 - val_loss: 0.4184 - val_auc_1: 0.9810\n",
            "Epoch 15/1000\n",
            "79/79 [==============================] - 1s 17ms/step - loss: 0.2406 - auc_1: 1.0000 - val_loss: 0.4232 - val_auc_1: 0.9812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f34bd8d0690>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "o4G566nAFs2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train,X_test,y_test = load_data(exclude = None)\n",
        "X_noise = X_test.copy()\n",
        "y_noise_pred = logistic_regression_model.predict(X_noise)\n",
        "plt.hist(y_noise_pred[:,-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "YTnGwi-iFu0b",
        "outputId": "1d665fc7-2d15-41ad-d8d6-b1ed62d89090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([515.,  93.,  63.,  38.,  52.,  48.,  44.,  73.,  71., 408.]),\n",
              " array([1.3438657e-17, 1.0000000e-01, 2.0000000e-01, 3.0000001e-01,\n",
              "        4.0000001e-01, 5.0000000e-01, 6.0000002e-01, 6.9999999e-01,\n",
              "        8.0000001e-01, 8.9999998e-01, 1.0000000e+00], dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5KaQ9D4wG3ij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}