{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "customized_training_with_noise_instances.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN07HMz2HBzNciEgGvFDp4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmningmei/simple_tensorflow_logistic_regression_classifier/blob/main/customized_training_with_noise_instances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This script adapts from [Customizing what happens in fit()](https://keras.io/guides/customizing_what_happens_in_fit/) for particular use in our lab"
      ],
      "metadata": {
        "id": "90PQh2trNsO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Cu_OPvIL-ihK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/simple_tensorflow_logistic_regression_classifier\"):\n",
        "    !git clone https://github.com/nmningmei/simple_tensorflow_logistic_regression_classifier.git\n",
        "\n",
        "os.chdir(\"/content/simple_tensorflow_logistic_regression_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0g-eE3J-qzq",
        "outputId": "8dd6d098-7676-408f-c722-a1c6b325055d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE       simple_logistic_regression_implemented_in_tensorflow.ipynb\n",
            "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/  test.py\n",
            "README.md     utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,models,initializers,optimizers,losses,metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "l7TMeW_6-tle"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# experiment control"
      ],
      "metadata": {
        "id": "ZaWNuc3R-wuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = int(1e3) # just a large number\n",
        "print_train = True\n",
        "batch_size = 8\n",
        "n_noise = 1 # number of noise inputs per epoch\n",
        "learning_rate = 1e-2\n",
        "tol = 1e-4\n",
        "patience = 10"
      ],
      "metadata": {
        "id": "zeuIAmCp-v-N"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clear memory states"
      ],
      "metadata": {
        "id": "lmtnWNhg-6D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "nrLCWDFK-375"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate some random data for classification"
      ],
      "metadata": {
        "id": "NvxHCHT2--4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = make_classification(n_samples             = 120,\n",
        "                          n_features            = 100,\n",
        "                          n_informative         = 3,\n",
        "                          n_redundant           = 10,\n",
        "                          n_classes             = 2,\n",
        "                          n_clusters_per_class  = 4,\n",
        "                          flip_y                = .01,\n",
        "                          class_sep             = .5,# how easy to separate the two classes\n",
        "                          shuffle               = True,\n",
        "                          random_state          = 12345,\n",
        "                          )"
      ],
      "metadata": {
        "id": "Im-3AE2O-8Sb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding for softmax"
      ],
      "metadata": {
        "id": "UfxkkzQz_HnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.reshape((-1,1))\n",
        "y = np.hstack([y,1-y]).astype('float32')"
      ],
      "metadata": {
        "id": "uVHDJUB8_Evd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split the data into train, validation, and test"
      ],
      "metadata": {
        "id": "9spg-dko_ON0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test   = train_test_split(X,y,test_size = .1,random_state = 12345)\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size = .1,random_state = 12345)"
      ],
      "metadata": {
        "id": "Le8tMmtE_J94"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build the model"
      ],
      "metadata": {
        "id": "vWPRmh7L_YlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "m2T8kv0CFwRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class simple_logistic_regression(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        This function is used during `fit()`\n",
        "\n",
        "        We manually add noise instances in each batch of training to increase\n",
        "        the diversity of the noise that the model sees.\n",
        "        \"\"\"\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "        x_mean,x_std = tf.nn.moments(x,axes = 0)\n",
        "        with tf.GradientTape() as tape:\n",
        "            x_noise = tf.random.normal(shape = (n_noise,x.shape[1]),\n",
        "                                       mean = x_mean,\n",
        "                                       stddev = x_std,\n",
        "                                       name = 'x_noise')\n",
        "            y_noise = tf.constant([[0.5,0.5]] * n_noise,dtype = \"float32\")\n",
        "            x_train = tf.concat([x,x_noise],0)\n",
        "            y_train = tf.concat([y,y_noise],0)\n",
        "            idx = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(idx)\n",
        "            x_train = tf.gather(x_train,idx,axis = 0)\n",
        "            y_train = tf.gather(y_train,idx,axis = 0)\n",
        "            y_pred = self(x_train, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y_train, y_pred, \n",
        "                                      regularization_losses = self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y_train, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    def test_step(self, data):\n",
        "        \"\"\"\n",
        "        We must have the testing function to avoid double dipping in testing\n",
        "        \"\"\"\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        # Compute predictions\n",
        "        y_pred = self(x, training=False)\n",
        "        # Updates the metrics tracking the loss\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "def build_model(input_size,output_size = 2,kernel_regularizer = None,activity_regularizer = None):\n",
        "    \"\"\"\n",
        "    This function builds the logistic regression classifier using the customized\n",
        "    modeling building method we define above\n",
        "\n",
        "    Inputs\n",
        "    ---\n",
        "    input_size: int, the 2nd dimension of the input features\n",
        "    output_size: int, default = 2\n",
        "    kernel_regularizer: None or keras.regularizers\n",
        "    activity_regularizer: None or keras.regularizers\n",
        "\n",
        "    Outputs\n",
        "    ---\n",
        "    model: keras.Models\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(12345)\n",
        "    input_layer = layers.Input(shape        = (input_size,),\n",
        "                                name         = \"input_layer\",)\n",
        "\n",
        "    logistic_layer = layers.Dense(units                 = output_size,\n",
        "                                  activation            = 'softmax',\n",
        "                                  use_bias              = True,\n",
        "                                  kernel_initializer    = initializers.HeNormal(),\n",
        "                                  kernel_regularizer    = kernel_regularizer,\n",
        "                                  activity_regularizer  = activity_regularizer,\n",
        "                                  name                  = 'logistic_layer'\n",
        "                                  )(input_layer)\n",
        "    model = simple_logistic_regression(input_layer,logistic_layer,\n",
        "                                       name = 'logistic_regression')\n",
        "    return model\n",
        "# the most important helper function: early stopping and model saving\n",
        "def make_CallBackList(model_name,monitor='val_loss',mode='min',verbose=0,min_delta=1e-4,patience=50,frequency = 1):\n",
        "    \n",
        "    \"\"\"\n",
        "    Make call back function lists for the keras models\n",
        "    \n",
        "    Parameters\n",
        "    -------------------------\n",
        "    model_name : str,\n",
        "        directory of where we want to save the model and its name\n",
        "    monitor : str, default = 'val_loss'\n",
        "        the criterion we used for saving or stopping the model\n",
        "    mode : str, default = 'min'\n",
        "        min --> lower the better, max --> higher the better\n",
        "    verboser : int or bool, default = 0\n",
        "        printout the monitoring messages\n",
        "    min_delta : float, default = 1e-4\n",
        "        minimum change for early stopping\n",
        "    patience : int, default = 50\n",
        "        temporal windows of the minimum change monitoring\n",
        "    frequency : int, default = 1\n",
        "        temporal window steps of the minimum change monitoring\n",
        "    \n",
        "    Return\n",
        "    --------------------------\n",
        "    CheckPoint : tensorflow.keras.callbacks\n",
        "        saving the best model\n",
        "    EarlyStopping : tensorflow.keras.callbacks\n",
        "        early stoppi\n",
        "    \"\"\"\n",
        "    checkPoint = ModelCheckpoint(model_name,# saving path\n",
        "                                 monitor          = monitor,# saving criterion\n",
        "                                 save_best_only   = True,# save only the best model\n",
        "                                 mode             = mode,# saving criterion\n",
        "                                 verbose          = verbose,# print out (>1) or not (0)\n",
        "                                 )\n",
        "    earlyStop = EarlyStopping(   monitor          = monitor,\n",
        "                                 min_delta        = min_delta,\n",
        "                                 patience         = patience,\n",
        "                                 verbose          = verbose, \n",
        "                                 mode             = mode,\n",
        "                                 )\n",
        "    return [checkPoint,earlyStop]\n",
        "def compile_logistic_regression(\n",
        "                    model,\n",
        "                    model_name      = 'temp.h5',\n",
        "                    optimizer       = None,\n",
        "                    loss_function   = None,\n",
        "                    metric          = None,\n",
        "                    callbacks       = None,\n",
        "                    learning_rate   = 1e-2,\n",
        "                    tol             = 1e-4,\n",
        "                    patience        = 5,\n",
        "                    ):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "    ---\n",
        "    model: tf.keras.models.Model or callable tf objects\n",
        "    model_name: str, directory of where we want to save the model and its name\n",
        "    optimizer: None or tf.keras.optimizers, default = SGD\n",
        "    loss_function: None or tf.keras.losses, default = BinaryCrossentropy\n",
        "    metric: None or tf.keras.metrics, default = AUC\n",
        "    callbacks: None or list of tf.keras.callbacks, default = [checkpoint,earlystopping]\n",
        "    learning_rate: float, learning rate, default = 1e-2,\n",
        "    tol: float, for determining when to stop training, default = 1e-4,\n",
        "    patience: int, for determing when to stop training, default = 5,\n",
        "    \n",
        "    Outputs\n",
        "    ---\n",
        "    model: tf.keras.models.Model or callable tf objects\n",
        "    callbacks:ist of tf.keras.callbacks\n",
        "    \"\"\"\n",
        "    if optimizer is None:\n",
        "        optimizer       = optimizers.SGD(learning_rate = learning_rate,)\n",
        "    if loss_function is None:\n",
        "        loss_function   = losses.BinaryCrossentropy()\n",
        "    if metric is None:\n",
        "        metric          = metrics.AUC()\n",
        "    if callbacks is None:\n",
        "        callbacks       = make_CallBackList(\n",
        "                                      model_name    = model_name,\n",
        "                                      monitor       = 'val_loss',\n",
        "                                      mode          = 'min',\n",
        "                                      verbose       = 0,\n",
        "                                      min_delta     = tol,\n",
        "                                      patience      = patience,\n",
        "                                      frequency     = 1,\n",
        "                                      )\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss      = loss_function,\n",
        "                  metrics   = [metric],\n",
        "                  run_eagerly = True\n",
        "                  )\n",
        "    return model,callbacks"
      ],
      "metadata": {
        "id": "rWZPDO8k_VRv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the helper functions to build the models"
      ],
      "metadata": {
        "id": "qsCPR1FZGGUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model = build_model(input_size = X.shape[1],output_size = 2,)\n",
        "logistic_regression_model,callbacks = compile_logistic_regression(logistic_regression_model,\n",
        "                                                                  learning_rate = learning_rate,\n",
        "                                                                  tol = tol,\n",
        "                                                                  patience = patience,\n",
        "                                                                  )"
      ],
      "metadata": {
        "id": "oCVp-Wu3E0si"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "cKG0hQGAL1Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model.fit(X_train,y_train,\n",
        "                              batch_size = batch_size,\n",
        "                              validation_data = (X_valid,y_valid),\n",
        "                              epochs = n_epochs,\n",
        "                              callbacks = callbacks,\n",
        "                              shuffle = True,\n",
        "                              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WNzUIHyEQHp",
        "outputId": "ac8f8d7f-bf84-4c62-b8d3-9c42951e1ba9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 0.9867 - auc: 0.3990 - val_loss: 0.8177 - val_auc: 0.5372\n",
            "Epoch 2/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.9359 - auc: 0.4254 - val_loss: 0.7918 - val_auc: 0.6033\n",
            "Epoch 3/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.8895 - auc: 0.4542 - val_loss: 0.7700 - val_auc: 0.6033\n",
            "Epoch 4/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.8637 - auc: 0.4852 - val_loss: 0.7533 - val_auc: 0.6116\n",
            "Epoch 5/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.8341 - auc: 0.5184 - val_loss: 0.7341 - val_auc: 0.6198\n",
            "Epoch 6/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.8111 - auc: 0.5502 - val_loss: 0.7207 - val_auc: 0.6198\n",
            "Epoch 7/1000\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 0.7836 - auc: 0.5750 - val_loss: 0.7080 - val_auc: 0.6446\n",
            "Epoch 8/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.7660 - auc: 0.6075 - val_loss: 0.6977 - val_auc: 0.6529\n",
            "Epoch 9/1000\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 0.7472 - auc: 0.6328 - val_loss: 0.6889 - val_auc: 0.6694\n",
            "Epoch 10/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.7306 - auc: 0.6544 - val_loss: 0.6840 - val_auc: 0.6860\n",
            "Epoch 11/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.7106 - auc: 0.6793 - val_loss: 0.6779 - val_auc: 0.6942\n",
            "Epoch 12/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.6963 - auc: 0.7013 - val_loss: 0.6711 - val_auc: 0.7025\n",
            "Epoch 13/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.6714 - auc: 0.7282 - val_loss: 0.6654 - val_auc: 0.7107\n",
            "Epoch 14/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.6625 - auc: 0.7391 - val_loss: 0.6605 - val_auc: 0.7273\n",
            "Epoch 15/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.6553 - auc: 0.7496 - val_loss: 0.6564 - val_auc: 0.7355\n",
            "Epoch 16/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.6417 - auc: 0.7634 - val_loss: 0.6528 - val_auc: 0.7355\n",
            "Epoch 17/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.6208 - auc: 0.7768 - val_loss: 0.6484 - val_auc: 0.7521\n",
            "Epoch 18/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.6099 - auc: 0.7910 - val_loss: 0.6453 - val_auc: 0.7686\n",
            "Epoch 19/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.6103 - auc: 0.7971 - val_loss: 0.6423 - val_auc: 0.7686\n",
            "Epoch 20/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5961 - auc: 0.8037 - val_loss: 0.6399 - val_auc: 0.7686\n",
            "Epoch 21/1000\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 0.5946 - auc: 0.8113 - val_loss: 0.6371 - val_auc: 0.7686\n",
            "Epoch 22/1000\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.5821 - auc: 0.8234 - val_loss: 0.6348 - val_auc: 0.7769\n",
            "Epoch 23/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.5651 - auc: 0.8351 - val_loss: 0.6327 - val_auc: 0.7769\n",
            "Epoch 24/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5620 - auc: 0.8366 - val_loss: 0.6310 - val_auc: 0.7769\n",
            "Epoch 25/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5584 - auc: 0.8381 - val_loss: 0.6294 - val_auc: 0.7769\n",
            "Epoch 26/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5465 - auc: 0.8441 - val_loss: 0.6286 - val_auc: 0.7769\n",
            "Epoch 27/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5395 - auc: 0.8531 - val_loss: 0.6249 - val_auc: 0.7769\n",
            "Epoch 28/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5401 - auc: 0.8498 - val_loss: 0.6234 - val_auc: 0.7769\n",
            "Epoch 29/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.5340 - auc: 0.8506 - val_loss: 0.6212 - val_auc: 0.7769\n",
            "Epoch 30/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5283 - auc: 0.8497 - val_loss: 0.6202 - val_auc: 0.7769\n",
            "Epoch 31/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5209 - auc: 0.8689 - val_loss: 0.6196 - val_auc: 0.7769\n",
            "Epoch 32/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.5066 - auc: 0.8751 - val_loss: 0.6190 - val_auc: 0.7769\n",
            "Epoch 33/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.5048 - auc: 0.8756 - val_loss: 0.6189 - val_auc: 0.7769\n",
            "Epoch 34/1000\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.5017 - auc: 0.8788 - val_loss: 0.6197 - val_auc: 0.7769\n",
            "Epoch 35/1000\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.4979 - auc: 0.8827 - val_loss: 0.6185 - val_auc: 0.7769\n",
            "Epoch 36/1000\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.4953 - auc: 0.8784 - val_loss: 0.6184 - val_auc: 0.7769\n",
            "Epoch 37/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4865 - auc: 0.8827 - val_loss: 0.6194 - val_auc: 0.7769\n",
            "Epoch 38/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4884 - auc: 0.8765 - val_loss: 0.6192 - val_auc: 0.7851\n",
            "Epoch 39/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4939 - auc: 0.8836 - val_loss: 0.6190 - val_auc: 0.7851\n",
            "Epoch 40/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4683 - auc: 0.9039 - val_loss: 0.6201 - val_auc: 0.7934\n",
            "Epoch 41/1000\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.4718 - auc: 0.8959 - val_loss: 0.6203 - val_auc: 0.7934\n",
            "Epoch 42/1000\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.4702 - auc: 0.9069 - val_loss: 0.6217 - val_auc: 0.7934\n",
            "Epoch 43/1000\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4688 - auc: 0.8993 - val_loss: 0.6213 - val_auc: 0.7934\n",
            "Epoch 44/1000\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.4606 - auc: 0.9004 - val_loss: 0.6207 - val_auc: 0.7934\n",
            "Epoch 45/1000\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 0.4536 - auc: 0.9118 - val_loss: 0.6222 - val_auc: 0.7934\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7bd379a710>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "Kc-7OSJTLmpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = logistic_regression_model.predict(X_test)"
      ],
      "metadata": {
        "id": "uXKXGF_FLmPX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'test score = {roc_auc_score(y_test,y_pred,):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfT7I_9AL4_L",
        "outputId": "58153ad1-899f-4a45-f079-300e22d6c5d3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test score = 0.8857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict on random data"
      ],
      "metadata": {
        "id": "nvUTYuM9MGxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_noise = np.random.normal(loc = 0,scale = 1,size = (100,X.shape[1]))"
      ],
      "metadata": {
        "id": "-Xgefw91L8kX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_noise_pred = logistic_regression_model.predict(X_noise)"
      ],
      "metadata": {
        "id": "Wd7b_D9ZMZex"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_noise_pred[:,-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "tvBi4cENMjhu",
        "outputId": "349a8b92-9544-444d-a068-1ab160129735"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([11.,  8., 10., 11.,  8.,  9., 11.,  9., 16.,  7.]),\n",
              " array([0.00345488, 0.10061055, 0.19776621, 0.29492188, 0.39207754,\n",
              "        0.4892332 , 0.5863889 , 0.6835445 , 0.7807002 , 0.87785584,\n",
              "        0.9750115 ], dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bdFenls6MqZK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}